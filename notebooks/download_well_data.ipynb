{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"download_well_data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"AbkkDj7YTQT1","colab_type":"code","outputId":"7383725e-0a7d-4728-d773-53661faffb35","executionInfo":{"status":"ok","timestamp":1585244104514,"user_tz":420,"elapsed":23317,"user":{"displayName":"David Dralle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi51Q1ER0dm1Lw4ne71R5ApC2aO7x8MQVyyUHxR2d4=s64","userId":"16732303765724091928"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R-BtVMWxTc4Z","colab_type":"code","colab":{}},"source":["%%bash\n","cp /gdrive/My\\ Drive/research/dendra-api-client-python/dendra_api_client.py .\n"," "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iuDPs8meIJbB","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"yvMBViyNMlTX","colab_type":"code","cellView":"form","colab":{}},"source":["#@title Dendra API\n","\n","'''\n","Dendra API Query\n","\n","Author: Collin Bode\n","Date: 2019-05-12\n","\n","Purpose: \n","Simplifies pulling data from the https://dendra.science time-series data management system.\n","Dendra API requires paging of records in sets of 2,016.  This library performs\n","that function automatically. \n","\n","NOTE: the 'get_datapoints' function, which is the primary reason for this library is quite slow. It will\n","be replaced in the next version when we have min.io set up on the server to handle very large requests.\n","\n","Parameters:\n","    query: a JSON object with the tags, organization, stations, and start/end times\n","    endpoint: what API endpoint to query. 'datapoints/lookup' (default), 'station','datastream','datapoint'\n","    interval: datalogger minutes between records, integer. 5 = ERCZO (default), 10 = UCNRS, 15 = USGS\n","'''\n","\n","import requests\n","import json\n","import pandas as pd\n","import datetime as dt\n","import pytz\n","from dateutil import tz\n","from dateutil.parser import parse\n","from getpass import getpass\n","import concurrent.futures\n","\n","# Params\n","#url = 'https://api.edge.dendra.science/v1/'  # version 1 of the API has been deprecated\n","url = 'https://api.edge.dendra.science/v2/'\n","headers = {\"Content-Type\":\"application/json\"}\n","\n","# Time Helper Functions\n","# These apply standardized formating and UTC conversion\n","def time_utc(str_time=\"\"):\n","    if(str_time == \"\"):\n","        dt_time = dt.datetime.now(pytz.utc)\n","    else:\n","        dt_time = parse(str_time)\n","        if(dt_time.tzinfo != pytz.utc):\n","            dt_time = dt_time.astimezone(pytz.utc)\n","    return dt_time\n","\n","def time_format(dt_time=dt.datetime.now()):\n","     str_time = dt.datetime.strftime(dt_time,\"%Y-%m-%dT%H:%M:%S\") # \"%Y-%m-%dT%H:%M:%S.%f\"\n","     return str_time\n","\n","def authenticate(email):\n","    data = {\n","        'email': email,\n","        'strategy': 'local',\n","        'password': getpass()\n","    }\n","    r = requests.post(url+'authentication', json=data)\n","    assert r.status_code == 201\n","    token = r.json()['accessToken']\n","    headers['Authorization'] = token\n","    \n","\n","# List Functions help find what you are looking for, do not retreive full metadata\n","def list_organizations(orgslug='all'):\n","    # options: 'erczo','ucnrs','chi'\n","    query = {\n","        '$sort[name]': 1,\n","        '$select[name]':1,\n","        '$select[slug]':1\n","    }\n","    if(orgslug != 'all'):\n","        query['slug'] = orgslug\n","    \n","    r = requests.get(url + 'organizations', headers=headers, params=query)\n","    assert r.status_code == 200\n","    rjson = r.json()\n","    return rjson['data']    \n","\n","def list_stations(orgslug='all',query_add='none'):\n","    # orgslug options: 'erczo','ucnrs','chi'\n","    # NOTE: can either do all orgs or one org. No option to list some,\n","    #       unless you custom add to the query.\n","    query = {\n","        '$sort[name]': 1,\n","        '$select[name]': 1,\n","        '$select[slug]': 1,\n","        '$limit': 2016\n","    }\n","\n","    # Narrow query to one organization\n","    if(orgslug != 'all'):\n","        org_list = list_organizations(orgslug)\n","        if(len(org_list) == 0): \n","            return 'ERROR: no organizations found with that acronym.'\n","        orgid = org_list[0]['_id'] \n","        query['organization_id'] = orgid\n","\n","    # Modify query adding custom elements\n","    if(query_add != 'none'):\n","        for element in query_add:\n","            query[element] = query_add[element]\n","\n","    # Request JSON from Dendra         \n","    r = requests.get(url + 'stations', headers=headers, params=query)\n","    assert r.status_code == 200\n","    rjson = r.json()\n","    return rjson['data']\n","\n","def list_datastreams_by_station_id(station_id,query_add = ''):\n","    query = {\n","        '$sort[name]': 1,\n","        '$select[name]': 1,\n","        'station_id': station_id,\n","        '$limit': 2016\n","    }\n","    if(query_add != ''):\n","        query.update(query_add)    \n","\n","    # Request JSON from Dendra         \n","    r = requests.get(url + 'datastreams', headers=headers, params=query)\n","    assert r.status_code == 200\n","    rjson = r.json()\n","    return rjson['data']\n","    \n","# translate SensorDB to Dendra ID\n","def get_datastream_id_from_dsid(dsid,orgslug='all',station_id = ''):\n","    # Legacy SensorDB used integer DSID (DatastreamID).  \n","    # This is a helper function to translate between Dendra datastream_id's and DSID's\n","    query = {'$limit':2016}\n","\n","    # Narrow query to one station\n","    if(station_id != ''):\n","        query.update({'station_id':station_id})\n","\n","    # Narrow query to one org or loop through all organizations\n","    org_list = list_organizations(orgslug)\n","    if(len(org_list) == 0): \n","        print('ERROR: no organizations found with that acronym.')\n","        return ''\n","    # Build list of metadata \n","    bigjson = {'data':[]}\n","    for org in org_list:\n","        orgid = org['_id']\n","        orgname = org['name']\n","        #print(orgname,orgid,query)\n","        query_org = query\n","        query_org.update({'organization_id': orgid})\n","        r = requests.get(url + 'datastreams', headers=headers, params=query)\n","        assert r.status_code == 200\n","        rjson = r.json()\n","        if(len(rjson['data']) > 0):\n","            bigjson['data'].extend(rjson['data'])\n","            #print(orgname,len(rjson['data']))\n","    dsid_list = []\n","    for ds in bigjson['data']:\n","        #print(ds['name'],ds['_id'])\n","        if('external_refs' not in ds):\n","            continue\n","        for ref in ds['external_refs']:\n","            if(ref['type'] == 'odm.datastreams.DatastreamID'):\n","                #print(\"\\t\",ref['type'], ref['identifier'])\n","                dsid_list.append([ref['identifier'],ds['_id']])\n","    for row in dsid_list:\n","        int_dsid = int(row[0])\n","        datastream_id = row[1]\n","        if(dsid == int_dsid):\n","            #print('FOUND!',dsid,int_dsid,datastream_id)\n","            return datastream_id\n","\n","\n","# GET Metadata returns full metadata\n","def get_datastream_by_id(datastream_id,query_add = ''): \n","    # deprecated use get_meta_\n","    rjson = get_meta_datastream_by_id(datastream_id,query_add)\n","    return rjson\n","    \n","def get_meta_datastream_by_id(datastream_id,query_add = ''):\n","    query = { '_id': datastream_id }\n","    if(query_add != ''):\n","        query.update(query_add)\n","    r = requests.get(url + 'datastreams', headers=headers, params=query)\n","    assert r.status_code == 200\n","    rjson = r.json()\n","    return rjson['data'][0]   \n","\n","def get_meta_station_by_id(station_id,query_add = ''):\n","    query = { '_id': station_id }\n","    if(query_add != ''):\n","        query.update(query_add)\n","    r = requests.get(url + 'stations', headers=headers, params=query)\n","    assert r.status_code == 200\n","    rjson = r.json()\n","    return rjson['data'][0]   \n","\n","\n","# GET Datapoints returns actual datavalues for only one datastream.  \n","# Returns a Pandas DataFrame columns. Both local and UTC time will be returned.\n","# Parameters: time_end is optional. Defaults to now. time_type is optional default 'local', either 'utc' or 'local' \n","# if you choose 'utc', timestamps must have 'Z' at the end to indicate UTC time.\n","\n","def get_datapoints(datastream_id,time_start,time_end=time_format(),time_type='local'):\n","    if(time_type == 'utc' and time_end[-1] != 'Z'):\n","        time_end += 'Z'\n","        \n","    query = {\n","        'datastream_id': datastream_id,\n","        'time[$gt]': time_start,\n","        'time[$lt]': time_end,\n","        '$sort[time]': \"1\",\n","        '$limit': \"2016\"\n","    } \n","    if(time_type != 'utc'): \n","        query.update({ 'time_local': \"true\" })\n","    \n","    # Dendra requires paging of 2,000 records maximum at a time.\n","    # To get around this, we loop through multiple requests and append\n","    # the results into a single dataset.\n","    try:\n","        r = requests.get(url + 'datapoints', headers=headers, params=query)\n","        assert r.status_code == 200\n","    except:\n","        return r.status_code\n","    rjson = r.json()\n","    bigjson = rjson\n","    while(len(rjson['data']) > 0):\n","        df = pd.DataFrame.from_records(bigjson['data'])\n","        time_last = df['lt'].max()\n","        query['time[$gt]'] = time_last\n","        r = requests.get(url + 'datapoints', headers=headers, params=query)\n","        assert r.status_code == 200\n","        rjson = r.json()\n","        bigjson['data'].extend(rjson['data'])\n","\n","    # Create Pandas DataFrame and set time as index\n","    # If the datastream has data for the time period, populate DataFrame\n","    if(len(bigjson['data']) > 0):\n","        df = pd.DataFrame.from_records(bigjson['data'])\n","    else:\n","        df = pd.DataFrame(columns={'lt','t','v'})\n","        \n","    # Get human readable name for data column\n","    datastream_meta = get_meta_datastream_by_id(datastream_id,{'$select[name]':1,'$select[station_id]':1})\n","    station_meta = get_meta_station_by_id(datastream_meta['station_id'],{'$select[slug]':1})\n","    stn = station_meta['slug'].replace('-',' ').title().replace(' ','')\n","    datastream_name = stn+'_'+datastream_meta['name'].replace(' ','_')\n","    \n","    # Rename columns, then set index to timestamp local or utc \n","    df.rename(columns={'lt':'timestamp_local','t':'timestamp_utc','v':datastream_name},inplace=True)\n","    if(time_type == 'utc'):\n","        df.set_index('timestamp_utc', inplace=True, drop=True)  \n","    else:\n","        df.set_index('timestamp_local', inplace=True, drop=True)\n","\n","    # Return DataFrame\n","    return df\n","\n","# GET Datapoints from List returns a dataframe of datapoints from a list of datastream ids. The function is \n","# threaded for speed.  List must be an array of text variables which are datastream ids.  The first datastream\n","# on the list will create the time-index, so it is best if this one is the most complete of the list. If it has \n","# time gaps, the rest of the dataframe can be comprimised.  This may need to be changes in the future.\n","# All requirements of above get_datapoints apply to get_datapoints_from_list.\n","def get_datapoints_from_id_list(datastream_id_list,time_start,time_end=time_format(),time_type='local'):\n","    i = 0\n","    boo_new = True\n","    dftemp_list = [] # list of dataframes from the results\n","\n","    with concurrent.futures.ThreadPoolExecutor() as executor:\n","        for dsid in datastream_id_list:\n","            i += 1\n","            future = executor.submit(get_datapoints,dsid,time_start,time_end,'local')\n","            dftemp_list.append(future)\n","\n","        for future in concurrent.futures.as_completed(dftemp_list):\n","            dftemp = future.result()\n","            #print(dftemp.columns)\n","\n","            # Check to see if any datapoints were returned.  \n","            # Many datastreams are not functional for the desired time frame.\n","            # If none, then skip the datastream and continue\n","            if(len(dftemp) == 0):\n","                    print(i,dftemp.columns[1],'No values, skipping...') \n","                    continue\n","            # If there are datapoints, check to see if the dataframe has been created yet. \n","            # If not, create, if so, add another column\n","            if(boo_new == True):\n","                df = dftemp\n","                boo_new = False\n","                print(i,dftemp.columns[1],'NEW dataframe created!')\n","            else:\n","                dftemp.drop(dftemp.columns[0],axis=1,inplace=True)\n","                df = df.merge(dftemp,how=\"left\",left_index=True,right_index=True)\n","                print(i,dftemp.columns[0],'added.')\n","    return df\n","\n","\n","# Lookup is an earlier attempt. Use get_datapoints unless you have to use this.    \n","def __lookup_datapoints_subquery(bigjson,query,endpoint='datapoints/lookup'):\n","    r = requests.get(url + endpoint, headers=headers, params=query)\n","    assert r.status_code == 200\n","    rjson = r.json()\n","    if(len(bigjson) == 0): # First pull assigns the metadata \n","        bigjson = rjson\n","    else:  # all others just add to the datapoints\n","        for i in range(0,len(bigjson)):\n","            bigjson[i]['datapoints']['data'].extend(rjson[i]['datapoints']['data'])\n","    return bigjson\n","\n","def lookup_datapoints(query,endpoint='datapoints/lookup',interval=5):    \n","    # Determine start and end timestamps\n","    # Start time\n","    #time_start_original = dt.datetime.strptime(query['time[$gte]'],'%Y-%m-%dT%H:%M:%SZ')\n","    time_start_original = parse(query['time[$gte]'])\n","    #time_start_original = pytz.utc.localize(time_start_original)\n","    # end time\n","    if('time[$lt]' in query):\n","        #time_end_original = dt.datetime.strptime(query['time[$lt]'],'%Y-%m-%dT%H:%M:%SZ')\n","        time_end_original = parse(query['time[$lt]'])\n","        #time_end_original = pytz.utc.localize(time_end_original)\n","    else: \n","        time_end_original_local = dt.datetime.now(tz.tzlocal())\n","        time_end_original = time_end_original_local.astimezone(pytz.utc)\n","    \n","    # Paging limit: 2016 records. \n","    interval2k = (dt.timedelta(minutes=interval) * 2016 )\n","\n","    # Perform repeat queries until the time_end catches up with the target end date\n","    time_start = time_start_original\n","    time_end = time_start_original+interval2k\n","    bigjson = {}\n","    while(time_end < time_end_original and time_start < time_end_original):    \n","        bigjson = __lookup_datapoints_subquery(bigjson,query,endpoint)\n","        time_start = time_end\n","        time_end = time_start+interval2k \n","    # One final pull after loop for the under 2016 records left\n","    bigjson = __lookup_datapoints_subquery(bigjson,query,endpoint)\n","\n","    # Count total records pulled and update limit metadata\n","    max_records = pd.date_range(start=time_start_original,end=time_end_original, tz='UTC',freq=str(interval)+'min')\n","    for i in range(0,len(bigjson)):\n","        bigjson[i]['datapoints']['limit'] = len(max_records) \n","\n","    # return the full metadata and records\n","    return bigjson\n","\n","\n","###############################################################################\n","# Unit Tests\n","#\n","def __main():\n","    btime = False\n","    borg = False\n","    bstation = False\n","    bdatastream_id = False\n","    bdatapoints = False\n","    bdatapoints_lookup = False    \n","\n","    ####################\n","    # Test Time\n","    if(btime == True):\n","        # time_utc converts string to datetime\n","        string_utc = '2019-03-01T08:00:00Z'\n","        print('UTC:',time_utc(string_utc))\n","        string_edt = '2019-03-01T08:00:00-0400'\n","        print('EDT:',time_utc(string_edt))\n","        string_hst = '2019-03-01T08:00:00HST'\n","        print('HST:',time_utc(string_hst))\n","        print('Empty (local default):',time_utc())\n","        \n","        # time_format converts datetime to utc string\n","        tu = dt.datetime.strptime(string_utc,'%Y-%m-%dT%H:%M:%SZ')\n","        print('time_format utc:',time_format(tu))\n","        te = dt.datetime.strptime(string_edt,'%Y-%m-%dT%H:%M:%S%z')\n","        print('time_format edt:',time_format(te))\n","        print('time_format empty:',time_format())\n","    \n","    \n","    ####################\n","    # Test Organizations\n","    if(borg == True):\n","        # Get One Organization ID\n","        erczo = list_organizations('erczo')\n","        print('Organizations ERCZO ID:',erczo[0]['_id'])\n","        \n","        # Get All Organization IDs        \n","        org_list = list_organizations()\n","        print('All Organizations:')\n","        print(\"ID\\t\\t\\tName\")\n","        for org in org_list:\n","            print(org['_id'],org['name'])\n","        \n","        # Send a BAD Organization slug\n","        orgs = list_organizations('Trump_is_Evil')\n","        print('BAD Organizations:',orgs)\n","    \n","    ####################    \n","    # Test stations\n","    if(bstation == True):\n","        # Get All stations\n","        st_list = list_stations()\n","        print('\\nALL Organization Stations\\n',st_list)\n","        \n","        # Get Stations from UCNRS only\n","        stslug = 'ucnrs'\n","        st_list = list_stations(stslug)\n","        #print(st_erczo)    \n","        print('\\n',stslug.upper(),'Stations\\n')\n","        print(\"ID\\t\\t\\tName\\t\\tSlug\")\n","        for station in st_list:\n","            print(station['_id'],station['name'],\"\\t\",station['slug'])\n","        \n","        # Modify Query\n","        query_add = {'$select[station_type]':1}\n","        print(query_add)\n","        st_list = list_stations(stslug) #,query_add)\n","        print('\\n',stslug.upper(),'Stations with station_type added\\n',st_list)    \n","    \n","        # What happens when you send a BAD organization string?\n","        st_list = list_stations('Trump is Evil')\n","        print('\\nBAD Organizations Stations\\n',st_list)\n","     \n","    ####################    \n","    # Test Datastream from id\n","    if(bdatastream_id == True):\n","        # Get all Metadata about one Datastream 'South Meadow WS, Air Temp C'        \n","        airtemp_id = '5ae8793efe27f424f9102b87'\n","        airtemp_meta = get_meta_datastream_by_id(airtemp_id)\n","        print(airtemp_meta)\n","        \n","        # Get only Name from Metadata using query_add\n","        airtemp_meta = get_meta_datastream_by_id(airtemp_id,{'$select[name]':1})\n","        print(airtemp_meta)\n","                \n","    ####################        \n","    # Test Datapoints \n","    if(bdatapoints == True):\n","        airtemp_id = '5ae8793efe27f424f9102b87'\n","        from_time = '2019-02-01T08:00:00.000Z' # UTC, not local PST time\n","        to_time = '2019-03-01T08:00:00Z'\n","        #to_time = None\n","        dd = get_datapoints(airtemp_id,from_time,to_time)\n","        dups = dd[dd.duplicated(keep=False)]\n","        print('get_datapoints count:',len(dd),'min date:',dd.index.min(),'max date:',dd.index.max())\n","        print('duplicates?\\n',dups)\n","        \n","        # No end date\n","        to_time = None\n","        dd = get_datapoints(airtemp_id,from_time)\n","        print('get_datapoints end date set to now, count:',len(dd),'min date:',dd.index.min(),'max date:',dd.index.max())\n","        print(dd)\n","        \n","    ####################        \n","    # Test Datapoints Lookup \n","    if(bdatapoints_lookup == True):\n","        # Parameters\n","        orgid = '58db17c424dc720001671378' # ucnrs\n","        station_id = '58e68cabdf5ce600012602b3'\n","        from_time = '2019-04-01T08:00:00.000Z' # UTC, not local PST time\n","        to_time = '2019-05-05T08:00:00Z'\n","        interval = 10 # 5,10,15\n","        \n","        tags = [\n","            'ds_Medium_Air',\n","            'ds_Variable_Temperature',\n","            'ds_Aggregate_Average'\n","        ]\n","        query = {\n","            'station_id': station_id,\n","            'time[$gte]': from_time,\n","            'tags': '.'.join(tags),\n","            '$sort[time]': 1,\n","            'time_local': 1,\n","            '$limit': 2000\n","        }\n","        if('to_time' in locals()):\n","        \tquery['time[$lt]'] = to_time\n","        #print(query)\n","        # Test the Query\n","        bigjson = lookup_datapoints(query,'datapoints/lookup',interval)\n","        \n","        # Show the results\n","        for doc in bigjson:\n","            print(doc['name'],len(doc['datapoints']['data']),doc['datapoints']['limit'],doc['_id'])\n","\n","if(__name__ == '__main__'):\n","    __main()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQjkLVky74wj","colab_type":"code","outputId":"d6352356-09dd-40b1-9197-e330368980cf","executionInfo":{"status":"ok","timestamp":1585244128785,"user_tz":420,"elapsed":8243,"user":{"displayName":"David Dralle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi51Q1ER0dm1Lw4ne71R5ApC2aO7x8MQVyyUHxR2d4=s64","userId":"16732303765724091928"}},"cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#@title authenticate \n","import dendra_api_client as dendra\n","# dendra.authenticate('daviddralle@gmail.com')\n","dendra.authenticate('daviddralle@gmail.com')\n","#Type in the password manually. It is rivendell2013 \n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["··········\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OjbBIGlT8L3I","colab_type":"code","outputId":"7eebf51f-f0e6-43b3-eefe-c716dc2cfee6","executionInfo":{"status":"ok","timestamp":1585204220611,"user_tz":420,"elapsed":3503785,"user":{"displayName":"David Dralle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi51Q1ER0dm1Lw4ne71R5ApC2aO7x8MQVyyUHxR2d4=s64","userId":"16732303765724091928"}},"colab":{"base_uri":"https://localhost:8080/","height":218}},"source":["well_ids = [\n","        '5ae8770dfe27f486fc102ac6',\n","        '5ae87703fe27f4c1d7102a93',\n","        '5ae87705fe27f4b750102a9f',\n","        '5ae8770bfe27f47fe1102ab8',\n","        '5ae8770cfe27f4e44a102abf',\n","        '5ae87706fe27f45d81102aa4',\n","        '5ae87707fe27f46339102aa7',\n","        '5ae87704fe27f429d2102a98',\n","        '5ae87712fe27f4dac7102ad9',\n","        '5ae87713fe27f4445c102ae0',\n","        '5ae87708fe27f4f0a9102aac',\n","        '5ae87709fe27f40708102ab1'\n","        ]\n","\n","from_time = '2009-01-01T20:10:00'\n","end_time = '2020-01-01T20:10:00'\n","df = dendra.get_datapoints_from_id_list(well_ids,from_time, end_time)\n","df.to_csv('/gdrive/My Drive/research/wells.csv')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["12 RivLevel23_Well_13_WaterLevel_m_Legacy NEW dataframe created!\n","12 RivLevel23_Well_14_Water_Level_m_Legacy added.\n","12 RivLevel12_Well_1_WaterLevel_m_Legacy added.\n","12 RivLevel41_Well_7_Water_Level_m_Legacy added.\n","12 RivLevel32_Well_10_Water_Level_m_Legacy added.\n","12 RivLevel12_Well_12_WaterLevel_m_Legacy added.\n","12 RivLevel41_Well_5_Water_Level_m added.\n","12 RivLevel41_Well_6_Water_Level_m_Legacy added.\n","12 RivLevel22_Well_3_Water_Level_m_Legacy added.\n","12 RivLevel22_Well_2_Water_Level_m_Legacy added.\n","12 RivLevel51_Well_15_Water_Level_m_Legacy added.\n","12 RivLevel51_Well_16_Water_Level_m_Legacy added.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s7mBUlAD6d2k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":506},"outputId":"6b64cb7a-ab08-4882-b9ca-e7ae178c0ea2","executionInfo":{"status":"error","timestamp":1585245916508,"user_tz":420,"elapsed":1723769,"user":{"displayName":"David Dralle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi51Q1ER0dm1Lw4ne71R5ApC2aO7x8MQVyyUHxR2d4=s64","userId":"16732303765724091928"}}},"source":["#level 5 TDR:\n","dsid_list = ['5ae8746cfe27f48062102735','5ae8746cfe27f41200102739','5ae8746dfe27f45ed010273d','5ae8746afe27f4665210272d','5ae8746bfe27f41993102731','5ae87680fe27f432d810284a','5ae87683fe27f4c4d810285a','5ae87688fe27f4df1c10286e']\n","\n","#level 3 TDR:\n","dsids = dsid_list + []\n","\n","from_time = '2009-01-01T20:10:00'\n","end_time = '2020-01-01T20:10:00'\n","df = dendra.get_datapoints_from_id_list(dsids,from_time, end_time)\n","df = df.resample('H').mean()\n","df.to_csv('/gdrive/My Drive/research/angelo_tdr_1H.csv')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["ERROR! Session/line number was not unique in database. History logging moved to new session 59\n","8 RivLevel51_Soil_Moisture_100cm NEW dataframe created!\n","8 RivLevel51_Soil_Moisture_35cm added.\n","8 RivLevel51_Soil_Moisture_15cm added.\n","8 RivLevel51_Soil_Moisture_70cm added.\n","8 RivLevel51_Soil_Moisture_138cm added.\n","8 RivLevel31_Soil_Moisture_L3S1_0.33 added.\n","8 RivLevel31_Soil_Moisture_L3S2_0.27 added.\n","8 RivLevel31_Soil_Moisture_L3S3_0.30 added.\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-778541b6a062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2020-01-01T20:10:00'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdendra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_datapoints_from_id_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrom_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'H'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive/My Drive/research/angelo_tdr_1H.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(self, rule, how, axis, fill_method, closed, label, convention, kind, loffset, limit, base, on, level)\u001b[0m\n\u001b[1;32m   8447\u001b[0m             \u001b[0mbase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8448\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8449\u001b[0;31m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8450\u001b[0m         )\n\u001b[1;32m   8451\u001b[0m         return _maybe_process_deprecations(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/resample.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(obj, kind, **kwds)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \"\"\"\n\u001b[1;32m   1305\u001b[0m     \u001b[0mtg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeGrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_resampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/resample.py\u001b[0m in \u001b[0;36m_get_resampler\u001b[0;34m(self, obj, kind)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0;34m\"Only valid with DatetimeIndex, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0;34m\"TimedeltaIndex or PeriodIndex, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             \u001b[0;34m\"but got an instance of %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m         )\n\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'Index'"]}]},{"cell_type":"code","metadata":{"id":"sn2UY6FwEB2y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":329},"outputId":"743f5f84-b95a-4cd5-b196-6cb854da2de9","executionInfo":{"status":"ok","timestamp":1585246644598,"user_tz":420,"elapsed":380,"user":{"displayName":"David Dralle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi51Q1ER0dm1Lw4ne71R5ApC2aO7x8MQVyyUHxR2d4=s64","userId":"16732303765724091928"}}},"source":["df.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>timestamp_utc</th>\n","      <th>RivLevel51_Soil_Moisture_100cm</th>\n","      <th>q_x</th>\n","      <th>RivLevel51_Soil_Moisture_35cm</th>\n","      <th>q_y</th>\n","      <th>RivLevel51_Soil_Moisture_15cm</th>\n","      <th>q_x</th>\n","      <th>RivLevel51_Soil_Moisture_70cm</th>\n","      <th>q_y</th>\n","      <th>RivLevel51_Soil_Moisture_138cm</th>\n","      <th>q</th>\n","      <th>RivLevel31_Soil_Moisture_L3S1_0.33</th>\n","      <th>RivLevel31_Soil_Moisture_L3S2_0.27</th>\n","      <th>RivLevel31_Soil_Moisture_L3S3_0.30</th>\n","    </tr>\n","    <tr>\n","      <th>timestamp_local</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2011-10-06T08:00:00.000</th>\n","      <td>2011-10-06T16:00:00.000Z</td>\n","      <td>0.205</td>\n","      <td>NaN</td>\n","      <td>0.236</td>\n","      <td>NaN</td>\n","      <td>0.135</td>\n","      <td>NaN</td>\n","      <td>0.181</td>\n","      <td>NaN</td>\n","      <td>0.284</td>\n","      <td>NaN</td>\n","      <td>0.009965</td>\n","      <td>0.018828</td>\n","      <td>0.015302</td>\n","    </tr>\n","    <tr>\n","      <th>2011-10-06T08:05:00.000</th>\n","      <td>2011-10-06T16:05:00.000Z</td>\n","      <td>0.205</td>\n","      <td>NaN</td>\n","      <td>0.236</td>\n","      <td>NaN</td>\n","      <td>0.135</td>\n","      <td>NaN</td>\n","      <td>0.181</td>\n","      <td>NaN</td>\n","      <td>0.284</td>\n","      <td>NaN</td>\n","      <td>0.009965</td>\n","      <td>0.018828</td>\n","      <td>0.015302</td>\n","    </tr>\n","    <tr>\n","      <th>2011-10-06T08:10:00.000</th>\n","      <td>2011-10-06T16:10:00.000Z</td>\n","      <td>0.205</td>\n","      <td>NaN</td>\n","      <td>0.236</td>\n","      <td>NaN</td>\n","      <td>0.135</td>\n","      <td>NaN</td>\n","      <td>0.181</td>\n","      <td>NaN</td>\n","      <td>0.284</td>\n","      <td>NaN</td>\n","      <td>0.009965</td>\n","      <td>0.018828</td>\n","      <td>0.015302</td>\n","    </tr>\n","    <tr>\n","      <th>2011-10-06T08:15:00.000</th>\n","      <td>2011-10-06T16:15:00.000Z</td>\n","      <td>0.205</td>\n","      <td>NaN</td>\n","      <td>0.236</td>\n","      <td>NaN</td>\n","      <td>0.135</td>\n","      <td>NaN</td>\n","      <td>0.181</td>\n","      <td>NaN</td>\n","      <td>0.284</td>\n","      <td>NaN</td>\n","      <td>0.009965</td>\n","      <td>0.018828</td>\n","      <td>0.015302</td>\n","    </tr>\n","    <tr>\n","      <th>2011-10-06T08:20:00.000</th>\n","      <td>2011-10-06T16:20:00.000Z</td>\n","      <td>0.205</td>\n","      <td>NaN</td>\n","      <td>0.235</td>\n","      <td>NaN</td>\n","      <td>0.135</td>\n","      <td>NaN</td>\n","      <td>0.181</td>\n","      <td>NaN</td>\n","      <td>0.284</td>\n","      <td>NaN</td>\n","      <td>0.009965</td>\n","      <td>0.018828</td>\n","      <td>0.015302</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                    timestamp_utc  ...  RivLevel31_Soil_Moisture_L3S3_0.30\n","timestamp_local                                    ...                                    \n","2011-10-06T08:00:00.000  2011-10-06T16:00:00.000Z  ...                            0.015302\n","2011-10-06T08:05:00.000  2011-10-06T16:05:00.000Z  ...                            0.015302\n","2011-10-06T08:10:00.000  2011-10-06T16:10:00.000Z  ...                            0.015302\n","2011-10-06T08:15:00.000  2011-10-06T16:15:00.000Z  ...                            0.015302\n","2011-10-06T08:20:00.000  2011-10-06T16:20:00.000Z  ...                            0.015302\n","\n","[5 rows x 14 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"dE-1OJraD_yb","colab_type":"code","colab":{}},"source":["\n","cols = [col for col in df.columns if 'q' not in col]\n","df = df[cols]\n","df = df.resample('H').mean()\n","\n","df.to_csv('/gdrive/My Drive/research/angelo_tdr_1H.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvZg7OUcMYq0","colab_type":"code","colab":{}},"source":["cols = [col for col in df.columns if 'q_' not in col]\n","df = df[cols]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7YCbJBdjy6i","colab_type":"code","outputId":"cc4471bf-9b0e-46c9-b4ff-94b60667e948","executionInfo":{"status":"ok","timestamp":1585235686285,"user_tz":420,"elapsed":3007,"user":{"displayName":"David Dralle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi51Q1ER0dm1Lw4ne71R5ApC2aO7x8MQVyyUHxR2d4=s64","userId":"16732303765724091928"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["lsd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TcO7ZAapV4Jg","colab_type":"code","colab":{}},"source":["\n","df.to_csv('/gdrive/My Drive/research/wells.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3KfqHqa5jXBu","colab_type":"code","outputId":"9b1a5f6a-a5d7-4386-9f71-8c55b37a554b","executionInfo":{"status":"error","timestamp":1585235696603,"user_tz":420,"elapsed":785,"user":{"displayName":"David Dralle","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi51Q1ER0dm1Lw4ne71R5ApC2aO7x8MQVyyUHxR2d4=s64","userId":"16732303765724091928"}},"colab":{"base_uri":"https://localhost:8080/","height":162}},"source":["df.resample('D').mean().to_csv('/gdrive/My Drive/research/wells_daily.csv')"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-45e944047e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive/My Drive/research/wells_daily.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","metadata":{"id":"phtupPrAju0C","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}